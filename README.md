# ğŸ“Š Jira AI Control Centre (Streamlit UI)

* An **AI-powered Jira dashboard** with an intuitive **web UI** built in **Streamlit**.
It combines Jiraâ€™s ticketing system with **local LLM analysis** to help teams **view, analyze, comment, and assign tickets** with ease.
* [Click here](https://ai-ops-control-centre-vn6kkepdzfbwbai7bbtjpf.streamlit.app/)
---

## ğŸš€ Features

### ğŸ“‹ Ticket Management

* **All Tickets** â€” View all Jira tickets in the project
* **Open Tickets** â€” Show tickets not marked as Done
* **Closed Tickets** â€” Show completed tickets

### ğŸ“ Ticket Actions

* **Summarize** â€” Display the summary & description of a ticket
* **Analyze** â€” AI-generated analysis of a ticket's content
* **Comment** â€” Post AI suggestions directly as a Jira comment
* **Suggest** â€” Get RAG-powered recommendations using similar past tickets

### ğŸ§‘â€ğŸ’¼ Assignment

* **Auto Assign** â€” Distribute unassigned tickets to least-loaded team members
* **Auto Assign Specific** â€” Assign a chosen ticket to the least-loaded user

### â“ Natural Language Queries

* Ask in plain English (e.g., *"Show me high priority tickets"*) and get filtered Jira results

### ğŸ”„ Live Updates

* **Refresh** â€” Reload Jira data without restarting the app

---

## ğŸ§  Tech Stack

* **Python 3.x**
* **Streamlit** (UI)
* **Jira REST API v3** (ticket management)
* **Ollama LLM** (local AI model for analysis)
* **FAISS + Sentence Transformers** (RAG for similar ticket retrieval)

---

## ğŸ›  Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/ai-ops-control-centre.git
cd ai-ops-control-centre
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure Jira & Model

Instead of storing credentials in `config.py`, store them in **Streamlit Secrets**:
Create a `.streamlit/secrets.toml` file:

```toml
EMAIL = "your-email@example.com"
API_TOKEN = "your-jira-api-token"
DOMAIN = "https://your-domain.atlassian.net"
PROJECT_KEY = "YOURPROJECT"
OLLAMA_MODEL = "mistral"

ASSIGNEE_EMAILS = [
    "user1@example.com",
    "user2@example.com"
]
```

---

## ğŸ’¬ Usage

Run the app locally:

```bash
streamlit run ui_app.py
```

Deploy to **Streamlit Cloud**:

* Push code to GitHub
* Link repo in [share.streamlit.io](https://share.streamlit.io)
* Add your credentials in Streamlit Cloud â†’ **Secrets**

---

## ğŸ“‚ Project Structure

```
ai-ops-control-centre/
â”œâ”€â”€ ui_app.py               # Streamlit UI app
â”œâ”€â”€ chatbot.py               # CLI version
â”œâ”€â”€ auto_assign.py           # Ticket assignment logic
â”œâ”€â”€ jira_client.py           # Jira API functions
â”œâ”€â”€ llm_chain.py             # AI + RAG processing
â”œâ”€â”€ rag_utils.py             # Similar ticket retrieval
â”œâ”€â”€ handle_natural_query.py  # Natural language â†’ JQL
â”œâ”€â”€ config.py                # Loads credentials (from secrets in UI)
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---
## ğŸ“ Authorâ€™s Note

This project is built using Ollama, meaning all AI features run locally on your machine. If you attempt to deploy this project (for example, on Streamlit Cloud or another hosting service), the AI-powered functionality will not work, since Ollama requires a local runtime environment.

Please note that the current Streamlit deployment is for educational purposes only â€” it demonstrates the appâ€™s structure and interface, but the AI components are inactive in that environment.

## ğŸ’¡ Scope for Improvement

* Integrate a Remote LLM API: Replace local Ollama calls with a cloud-based API (e.g., OpenAI, Anthropic, or Hugging Face) to enable online deployment.

* Add a Configurable Backend: Let users toggle between local (Ollama) and remote (API-based) modes using environment variables.

* Use a Proxy Service: Run a small local server that communicates with Ollama and can be connected to a remote app via secure tunneling (e.g., ngrok).

* Model Management Tools: Include scripts to easily download or switch between Ollama models.

* Improved Setup Documentation: Provide step-by-step installation and configuration instructions for both local and educational (deployed) use cases.

## ğŸ‘©â€ğŸ’» Author

**Saumya Mathur**

[LinkedIn](https://www.linkedin.com/in/saumya-mathur) | [GitHub](https://github.com/saumyaya)

---
